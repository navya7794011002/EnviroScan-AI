# -*- coding: utf-8 -*-
"""EviroScan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vr26jGxLqmfP_B3MYz45LKlViplMuVNP

1.Create venv & install deps:
"""

!python -m venv venv

"""2.Generate datasets (2000 rows each), train baseline models, create map:"""

!python src/setup_generate_data_and_train.py

"""3.(Optional) Retrain with both RF & XGB"""

!python src/module4_model_training.py

"""4.Start the Streamlit realtime dashboard:"""

!streamlit run src/module6_dashboard_streamlit.py

"""1).requirements.txt"""

!pip install pandas numpy scikit-learn xgboost joblib folium streamlit geopy python-dotenv osmnx geopandas shapely requests matplotlib seaborn sendgrid twilio

!pip install osmnx

!pip install geopandas

"""2) .env.example"""

# Store sensitive information like API keys in Colab's Secrets Manager (the key icon in the left sidebar).
# Give your secrets names (e.g., 'OPENWEATHER_API_KEY', 'SENDGRID_API_KEY', 'TWILIO_ACCOUNT_SID').
# Then, you can access them in your code like this:

from google.colab import userdata

# Example of accessing a secret:
# openweather_api_key = userdata.get('OPENWEATHER_API_KEY')
# openweather_url = userdata.get('OPENWEATHER_URL') # If you also store the URL as a secret

# Or if you need to set environment variables for shell commands:
# import os
# os.environ['OPENWEATHER_API_KEY'] = userdata.get('OPENWEATHER_API_KEY')
# !echo $OPENWEATHER_API_KEY # Example of using the environment variable in a shell command

# Note: The original content in this cell was not valid Python syntax for variable assignment.
# If you need to use a .env file approach, you would need to install python-dotenv and load the file.
# !pip install python-dotenv
# from dotenv import load_dotenv
# load_dotenv('.env') # Assuming your .env file is in the current directory

"""3) Module 0 ‚Äî Setup & Synthetic data generation + baseline **training**"""

# src/setup_generate_data_and_train.py
import os
from pathlib import Path
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import joblib
import folium
from folium.plugins import HeatMap
import json

# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory

RAW = BASE / "data" / "raw"
PROCESSED = BASE / "data" / "processed"
MODELS = BASE / "data" / "models"
MAPS = BASE / "data" / "maps"
for p in [RAW, PROCESSED, MODELS, MAPS]:
    p.mkdir(parents=True, exist_ok=True)

np.random.seed(42)

# 1) Create base ~300 monitoring locations
n_base = 300
center_lat, center_lon = 17.3850, 78.4867  # Hyderabad approx center
lats = center_lat + np.random.normal(scale=0.1, size=n_base)
lons = center_lon + np.random.normal(scale=0.1, size=n_base)
locations = [f"Loc_{i}" for i in range(n_base)]

base = pd.DataFrame({
    "location": locations,
    "latitude": lats,
    "longitude": lons
})

# 2) Sample pollutant distributions (positive)
def sample_pollutants(n):
    pm25 = np.abs(np.random.normal(loc=60, scale=45, size=n))
    pm10 = np.abs(np.random.normal(loc=80, scale=60, size=n))
    no2  = np.abs(np.random.normal(loc=30, scale=20, size=n))
    so2  = np.abs(np.random.normal(loc=10, scale=8, size=n))
    co   = np.abs(np.random.normal(loc=0.8, scale=0.5, size=n))
    o3   = np.abs(np.random.normal(loc=20, scale=15, size=n))
    return pm25, pm10, no2, so2, co, o3

pm25, pm10, no2, so2, co, o3 = sample_pollutants(n_base)
base["pm25"] = pm25
base["pm10"] = pm10
base["no2"] = no2
base["so2"] = so2
base["co"] = co
base["o3"] = o3

# 3) timestamps & time features
timestamps = pd.date_range(end=pd.Timestamp.now(), periods=n_base, freq="H")
base["timestamp_utc"] = np.random.choice(timestamps, size=n_base, replace=False)
base["timestamp_utc"] = pd.to_datetime(base["timestamp_utc"])
base["hour"] = base["timestamp_utc"].dt.hour
base["dayofweek"] = base["timestamp_utc"].dt.dayofweek

# 4) weather + distance mocks
base["temp"] = np.random.normal(loc=28, scale=4, size=n_base)
base["humidity"] = np.clip(np.random.normal(loc=55, scale=15, size=n_base), 10, 100)
base["wind_speed"] = np.abs(np.random.normal(loc=2.5, scale=1.2, size=n_base))
base["dist_to_road"] = np.abs(np.random.normal(loc=500, scale=700, size=n_base)) + 10
base["dist_to_industry"] = np.abs(np.random.normal(loc=2000, scale=1500, size=len(sample) if 'sample' in locals() else n_base)) + 10

base.to_csv(RAW / "openaq_base.csv", index=False)

# 5) Expand/augment to target ~2000 rows
def expand_to_target(df, target=2000, seed=42):
    np.random.seed(seed)
    parts = [df]
    current = len(df)
    while current < target:
        sample = df.sample(n=min(len(df), target-current)).copy()
        # jitter spatially and numerically
        sample["latitude"] += np.random.normal(scale=0.005, size=len(sample))
        sample["longitude"] += np.random.normal(scale=0.005, size=len(sample))
        for col in ["pm25","pm10","no2","so2","co","o3","temp","humidity","wind_speed"]:
            sample[col] = sample[col] * (1 + np.random.normal(scale=0.08, size=len(sample)))
        sample["timestamp_utc"] = pd.to_datetime(sample["timestamp_utc"]) + pd.to_timedelta(np.random.randint(-12,13,size=len(sample)), unit="h")
        sample["hour"] = sample["timestamp_utc"].dt.hour
        sample["dayofweek"] = sample["timestamp_utc"].dt.dayofweek
        sample["dist_to_road"] = np.abs(sample["dist_to_road"] * (1 + np.random.normal(scale=0.2, size=len(sample)))) + 5
        sample["dist_to_industry"] = np.abs(sample["dist_to_industry"] * (1 + np.random.normal(scale=0.25, size=len(sample)))) + 5
        parts.append(sample)
        current += len(sample)
    out = pd.concat(parts, ignore_index=True).head(target)
    return out

df_aq = expand_to_target(base, 2000)
df_aq.to_csv(RAW / "openaq_city_2000.csv", index=False)

# Weather file aligned (simple copy)
df_weather = df_aq[["latitude","longitude","temp","humidity","wind_speed","hour","dayofweek"]].copy()
df_weather.to_csv(RAW / "weather_points_2000.csv", index=False)

# 6) Feature engineering (scaled pollutants)
polls = ["pm25","pm10","no2","so2","co","o3"]
scaler = StandardScaler()
df_aq[[p + "_s" for p in polls]] = scaler.fit_transform(df_aq[polls].fillna(0))
PROCESSED.mkdir(parents=True, exist_ok=True)
df_aq.to_csv(PROCESSED / "features_prelabel_2000.csv", index=False)

# 7) Heuristic labeling rules
def label_row(r):
    if (r["dist_to_industry"] < 1000) and (r["so2"] > 20):
        return "Industrial"
    if (r["dist_to_road"] < 250) and (r["no2"] > 35):
        return "Vehicular"
    if (r["pm25"] > 150) and (r["hour"] in [6,7,17,18]):
        return "Burning"
    if (r["pm25"] > 100) and (r["pm10"] > 150):
        return "Agricultural"
    return "Natural"

df_aq["pollution_source"] = df_aq.apply(label_row, axis=1)
df_aq.to_csv(PROCESSED / "labeled_dataset_2000.csv", index=False)

# 8) Train RF & XGB, evaluate, and save best model(s)
features = ["pm25_s","pm10_s","no2_s","so2_s","co_s","o3_s","temp","humidity","wind_speed","hour","dayofweek","dist_to_road","dist_to_industry"]
X = df_aq[features].fillna(0)
le = LabelEncoder()
y = le.fit_transform(df_aq["pollution_source"])

# RandomForest
rf = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)
rf.fit(X, y)
joblib.dump({"model": rf, "label_encoder": le, "features": features}, MODELS / "best_model_rf.joblib")

# XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric="mlogloss", random_state=42)
xgb.fit(X, y)
joblib.dump({"model": xgb, "label_encoder": le, "features": features}, MODELS / "best_model_xgb.joblib")

# Simple evaluation (train-split metrics)
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
preds_rf = rf.predict(Xte)
preds_xgb = xgb.predict(Xte)
rep_rf = classification_report(yte, preds_rf, output_dict=True)
rep_xgb = classification_report(yte, preds_xgb, output_dict=True)

report = {"rf": rep_rf, "xgb": rep_xgb}
with open(MODELS / "model_report.json", "w") as f:
    json.dump(report, f, indent=2)

# 9) Create folium heatmap (map used by dashboard)
center_lat = df_aq["latitude"].mean()
center_lon = df_aq["longitude"].mean()
m = folium.Map(location=[center_lat, center_lon], zoom_start=11)
heat_data = df_aq[["latitude","longitude","pm25"]].dropna().values.tolist()
HeatMap([[r[0], r[1], r[2]] for r in heat_data], radius=10, max_zoom=13, min_opacity=0.3).add_to(m)
color_map = {"Industrial":"red","Vehicular":"blue","Agricultural":"green","Burning":"orange","Natural":"gray"}
for _, r in df_aq.sample(300, random_state=1).iterrows():
    folium.CircleMarker(
        location=[r["latitude"], r["longitude"]],
        radius=4,
        color=color_map.get(r["pollution_source"], "gray"),
        popup=f"Source:{r['pollution_source']}<br>pm25:{r['pm25']:.1f}"
    ).add_to(m)
MAPS.mkdir(parents=True, exist_ok=True)
m.save(MAPS / "dashboard_map.html")

print("Done. Created datasets, models, and map.")

"""4) Module 1 ‚Äî Data collection (offline loader)"""

# src/module1_data_collection.py
from pathlib import Path
import pandas as pd
import os

# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory
RAW = BASE / "data" / "raw"

def load_openaq_local(filename="openaq_city_2000.csv"):
    return pd.read_csv(RAW / filename)

def load_weather_local(filename="weather_points_2000.csv"):
    return pd.read_csv(RAW / filename)

if __name__ == "__main__":
    aq = load_openaq_local()
    w = load_weather_local()
    print("AQ rows:", len(aq))
    print("Weather rows:", len(w))

"""5) Module 1b ‚Äî Live ingestion (optional)"""

OPENAQ_URL = os.getenv("OPENAQ_API_URL", "https://api.openaq.org")

!pip install python-dotenv pandas requests

!echo "OPENWEATHER_API_KEY="47d01273f50c456dc3e575754dd3d7d2" > .env

# Colab-ready module: Live AQI + Weather Collection

import os, time, requests
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv

# --- Setup ---
load_dotenv()  # Load API keys from .env

BASE = Path(os.getcwd())         # Base directory in Colab
RAW = BASE / "data" / "raw"     # Folder to save raw CSV
RAW.mkdir(parents=True, exist_ok=True)

# --- API URLs ---
OPENAQ_URL = "https://api.openaq.org/v2/measurements"
OWM_URL = "https://api.openweathermap.org/data/2.5/weather"
OWM_KEY = os.getenv("OWM_KEY")   # Correct way to read key

# --- Functions ---

def fetch_openaq(city="Hyderabad", country="IN", limit=100, page=1):
    """Fetch latest air quality measurements from OpenAQ"""
    params = {
        "city": city,
        "country": country,
        "limit": limit,
        "page": page
    }
    r = requests.get(OPENAQ_URL, params=params, timeout=20)
    r.raise_for_status()
    return r.json()

def fetch_weather(city="Hyderabad"):
    """Fetch current weather from OpenWeatherMap"""
    params = {"q": city, "appid": OWM_KEY, "units": "metric"}
    r = requests.get(OWM_URL, params=params, timeout=10)
    r.raise_for_status()
    return r.json()

def collect_air_quality(city="Hyderabad", pages=1, out_prefix="openaq_live"):
    """Collect AQI data and save as CSV"""
    rows = []
    for p in range(1, pages + 1):
        try:
            j = fetch_openaq(city=city, page=p, limit=100)
        except requests.HTTPError as e:
            print(f"Failed to fetch page {p}: {e}")
            continue

        for rec in j.get("results", []):
            coords = rec.get("coordinates", {})
            for m in rec.get("measurements", []):
                rows.append({
                    "location": rec.get("location"),
                    "parameter": m.get("parameter"),
                    "value": m.get("value"),
                    "unit": m.get("unit"),
                    "latitude": coords.get("latitude"),
                    "longitude": coords.get("longitude"),
                    "timestamp_utc": m.get("lastUpdated"),
                    "raw": m
                })
        time.sleep(1)  # polite pause

    df = pd.DataFrame(rows)
    out = RAW / f"{out_prefix}_{city.replace(' ', '_')}.csv"
    df.to_csv(out, index=False)
    print("Saved AQI data to", out)
    return df

def collect_weather(city="Hyderabad", out_prefix="weather_live"):
    """Collect weather data and save as CSV"""
    try:
        data = fetch_weather(city)
    except requests.HTTPError as e:
        print(f"Failed to fetch weather: {e}")
        return None

    row = {
        "city": city,
        "temperature_C": data.get("main", {}).get("temp"),
        "humidity": data.get("main", {}).get("humidity"),
        "pressure": data.get("main", {}).get("pressure"),
        "weather_main": data.get("weather", [{}])[0].get("main"),
        "weather_description": data.get("weather", [{}])[0].get("description"),
        "wind_speed": data.get("wind", {}).get("speed"),
        "wind_deg": data.get("wind", {}).get("deg"),
        "timestamp_utc": pd.Timestamp.utcnow(),
        "raw": data
    }
    df = pd.DataFrame([row])
    out = RAW / f"{out_prefix}_{city.replace(' ', '_')}.csv"
    df.to_csv(out, index=False)
    print("Saved weather data to", out)
    return df

# --- Main Execution ---
if __name__ == "__main__":
    city_name = "Hyderabad"

    print("Collecting Air Quality Data...")
    aqi_df = collect_air_quality(city=city_name, pages=1)

    print("\nCollecting Weather Data...")
    weather_df = collect_weather(city=city_name)

    print("\nDone! Data saved in:", RAW)

"""6) Module 2 ‚Äî Cleaning & Feature Engineering"""

# src/module2_cleaning_feature_engineering.py
from pathlib import Path
import pandas as pd
from sklearn.preprocessing import StandardScaler
import os # Import the os module

# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory

RAW = BASE / "data" / "raw"
PROCESSED = BASE / "data" / "processed"

def build_features(aq_filename="openaq_city_2000.csv", weather_filename="weather_points_2000.csv"):
    aq = pd.read_csv(RAW / aq_filename)
    weather = pd.read_csv(RAW / weather_filename)
    # approximate join by rounding coordinates
    aq["lat_round"] = aq["latitude"].round(3)
    aq["lon_round"] = aq["longitude"].round(3)
    weather["lat_round"] = weather["latitude"].round(3)
    weather["lon_round"] = weather["longitude"].round(3)
    merged = aq.merge(weather.drop(columns=["latitude","longitude"]), on=["lat_round","lon_round"], how="left", suffixes=("","_w"))
    # fill / ensure columns
    for c in ["temp","humidity","wind_speed","hour","dayofweek"]:
        if c not in merged.columns:
            merged[c] = 0
        merged[c] = merged[c].fillna(merged[c].median())
    pollutants = ["pm25","pm10","no2","so2","co","o3"]
    for p in pollutants:
        if p not in merged.columns:
            merged[p] = 0.0
    scaler = StandardScaler()
    merged[[p + "_s" for p in pollutants]] = scaler.fit_transform(merged[pollutants].fillna(0))
    PROCESSED.mkdir(parents=True, exist_ok=True)
    merged.to_csv(PROCESSED / "features_prelabel_2000.csv", index=False)
    print("Saved features to:", PROCESSED / "features_prelabel_2000.csv")

if __name__ == "__main__":
    build_features()

"""7) Module 3 ‚Äî Labeling & Augmentation"""

# src/module3_labeling_simulation.py
from pathlib import Path
import pandas as pd
import os # Import the os module

# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory

PROCESSED = BASE / "data" / "processed"

def label_rules(df):
    def label_row(r):
        if (r.get("dist_to_industry", 1e9) < 1000) and (r.get("so2", 0) > 20):
            return "Industrial"
        if (r.get("dist_to_road", 1e9) < 250) and (r.get("no2", 0) > 35):
            return "Vehicular"
        if (r.get("pm25", 0) > 150) and (r.get("hour", 0) in [6,7,17,18]):
            return "Burning"
        if (r.get("pm25", 0) > 100) and (r.get("pm10", 0) > 150):
            return "Agricultural"
        return "Natural"
    df["pollution_source"] = df.apply(label_row, axis=1)
    return df

if __name__ == "__main__":
    df = pd.read_csv(PROCESSED / "features_prelabel_2000.csv")
    df = label_rules(df)
    df.to_csv(PROCESSED / "labeled_dataset_2000.csv", index=False)
    print("Saved labeled dataset:", PROCESSED / "labeled_dataset_2000.csv")

"""8) Module 4 ‚Äî Model training (RF & XGB) + selection"""

# src/module4_model_training.py
from pathlib import Path
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import json, numpy as np
import os # Import the os module

# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory

PROCESSED = BASE / "data" / "processed"
MODELS = BASE / "data" / "models"

def train_and_save():
    df = pd.read_csv(PROCESSED / "labeled_dataset_2000.csv")
    features = ["pm25_s","pm10_s","no2_s","so2_s","co_s","o3_s","temp","humidity","wind_speed","hour","dayofweek","dist_to_road","dist_to_industry"]
    for f in features:
        if f not in df.columns:
            df[f] = 0.0
    X = df[features].fillna(0)
    le = LabelEncoder()
    y = le.fit_transform(df["pollution_source"])
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # RandomForest
    rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    preds_rf = rf.predict(X_test)
    report_rf = classification_report(y_test, preds_rf, output_dict=True)
    cm_rf = confusion_matrix(y_test, preds_rf).tolist()
    joblib.dump({"model": rf, "label_encoder": le, "features": features}, MODELS / "best_model_rf.joblib")

    # XGBoost
    xgb = XGBClassifier(use_label_encoder=False, eval_metric="mlogloss", random_state=42)
    xgb.fit(X_train, y_train)
    preds_xgb = xgb.predict(X_test)
    report_xgb = classification_report(y_test, preds_xgb, output_dict=True)
    cm_xgb = confusion_matrix(y_test, preds_xgb).tolist()
    joblib.dump({"model": xgb, "label_encoder": le, "features": features}, MODELS / "best_model_xgb.joblib")

    # Save JSON report comparing models
    out = {
        "rf": {"report": report_rf, "confusion_matrix": cm_rf},
        "xgb": {"report": report_xgb, "confusion_matrix": cm_xgb}
    }
    with open(MODELS / "model_comparison_report.json", "w") as f:
        json.dump(out, f, indent=2)

    # Print summary
    for name, rep in [("RF", report_rf), ("XGB", report_xgb)]:
        print(f"=== {name} macro avg f1:", rep.get("macro avg", {}).get("f1-score"))

if __name__ == "__main__":
    train_and_save()

"""9) Module 4b ‚Äî Evaluation script (optional notebook/script)"""

# src/module4_evaluate.py
from pathlib import Path
import pandas as pd, joblib
from sklearn.metrics import classification_report, confusion_matrix
import json
import os # Import the os module

# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory

PROCESSED = BASE / "data" / "processed"
MODELS = BASE / "data" / "models"

def evaluate(model_path):
    art = joblib.load(model_path)
    model = art["model"]
    le = art["label_encoder"]
    features = art["features"]
    df = pd.read_csv(PROCESSED / "labeled_dataset_2000.csv")
    X = df[features].fillna(0)
    y = le.transform(df["pollution_source"])
    preds = model.predict(X)
    rep = classification_report(y, preds, target_names=le.classes_, output_dict=True)
    cm = confusion_matrix(y, preds).tolist()
    return rep, cm, le.classes_

if __name__ == "__main__":
    for m in ["best_model_rf.joblib", "best_model_xgb.joblib"]:
        # Ensure the full path is passed to evaluate
        rep, cm, classes = evaluate(MODELS / m)
        print(m, "macro f1:", rep["macro avg"]["f1-score"])
        with open(MODELS / f"eval_{m.replace('.joblib','.json')}", "w") as f:
            json.dump({"report": rep, "confusion_matrix": cm, "classes": list(classes)}, f, indent=2)

"""10) Module 5 ‚Äî Geospatial mapping (folium heatmap)"""

# src/module5_geospatial_mapping.py
import pandas as pd
from pathlib import Path
import folium
from folium.plugins import HeatMap
import os # Import the os module

# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory

PROCESSED = BASE / "data" / "processed"
MAPS = BASE / "data" / "maps"

def create_heatmap(infile="labeled_dataset_2000.csv", out_html="dashboard_map.html"):
    df = pd.read_csv(PROCESSED / infile)
    center_lat = df["latitude"].mean()
    center_lon = df["longitude"].mean()
    m = folium.Map(location=[center_lat, center_lon], zoom_start=11)
    heat_data = df[["latitude","longitude","pm25"]].dropna().values.tolist()
    HeatMap([[r[0], r[1], r[2]] for r in heat_data], radius=8, max_zoom=13, min_opacity=0.3).add_to(m)
    color_map = {"Industrial":"red","Vehicular":"blue","Agricultural":"green","Burning":"orange","Natural":"gray"}
    for _, r in df.sample(200, random_state=2).iterrows():
        folium.CircleMarker(
            location=[r["latitude"], r["longitude"]],
            radius=4,
            color=color_map.get(r.get("pollution_source", "Natural"), "gray"),
            popup=f"Source:{r.get('pollution_source')}<br>pm25:{r.get('pm25'):.1f}"
        ).add_to(m)
    MAPS.mkdir(parents=True, exist_ok=True)
    out = MAPS / out_html
    m.save(out)
    print("Saved map to:", out)
    return out

if __name__ == "__main__":
    create_heatmap()

"""11) Module 5b ‚Äî Spatial distances using OSMnx (optional)"""

# Colab-ready: Spatial Distance Computation (roads & industrial areas)

import pandas as pd
from pathlib import Path
import os
import osmnx as ox
import geopandas as gpd

# --- Paths ---
BASE = Path(os.getcwd())  # works in Colab
PROCESSED = BASE / "data" / "processed"
PROCESSED.mkdir(parents=True, exist_ok=True)

# --- Function ---
def compute_nearest_distances(infile="features_prelabel_2000.csv", radius=3000):
    """
    Compute nearest distances from input points to nearest road and industrial landuse
    using OpenStreetMap data via OSMnx.
    """
    # Load CSV
    df = pd.read_csv(PROCESSED / infile)
    gdf_pts = gpd.GeoDataFrame(
        df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs="EPSG:4326"
    )
    gdf_pts = gdf_pts.to_crs(epsg=3857)  # project to meters

    # Center of dataset for OSM query
    center_lat, center_lon = df.latitude.mean(), df.longitude.mean()
    print(f"Downloading OSM geometry within {radius}m of ({center_lat:.4f}, {center_lon:.4f})...")

    # Roads
    roads = ox.features_from_point(
        (center_lat, center_lon), tags={"highway": True}, dist=radius
    )

    # Industrial zones
    ind = ox.features_from_point(
        (center_lat, center_lon), tags={"landuse": "industrial"}, dist=radius
    )

    # Reproject OSM layers
    if not roads.empty: roads = roads.to_crs(epsg=3857)
    if not ind.empty: ind = ind.to_crs(epsg=3857)

    # --- Distances ---
    if not roads.empty:
        nearest_roads = gdf_pts.sjoin_nearest(roads[["geometry"]], how="left", distance_col="dist_to_road")
        df["dist_to_road"] = nearest_roads["dist_to_road"]
    else:
        df["dist_to_road"] = float("nan")

    if not ind.empty:
        nearest_ind = gdf_pts.sjoin_nearest(ind[["geometry"]], how="left", distance_col="dist_to_industry")
        df["dist_to_industry"] = nearest_ind["dist_to_industry"]
    else:
        df["dist_to_industry"] = float("nan")

    # Save
    outfile = PROCESSED / infile.replace(".csv", "_with_distances.csv")
    df.to_csv(outfile, index=False)
    print("‚úÖ Saved distances to:", outfile)
    return df

# --- Run Example ---
if __name__ == "__main__":
    compute_nearest_distances()

"""12) Module 6 ‚Äî Streamlit realtime dashboard"""

# src/module6_dashboard_streamlit.py
import streamlit as st
import pandas as pd
from pathlib import Path
import joblib
import streamlit.components.v1 as components
from dotenv import load_dotenv
import os

load_dotenv()
# Determine the base path using the current working directory
# BASE = Path(__file__).resolve().parents[1] # Original line
BASE = Path(os.getcwd()) # Modified line to use current working directory

PROCESSED = BASE / "data" / "processed"
MODELS = BASE / "data" / "models"
MAPS = BASE / "data" / "maps"

st.set_page_config(page_title="AI-EnviroScan Dashboard", layout="wide")

@st.cache_data
def load_data():
    return pd.read_csv(PROCESSED / "features_prelabel_2000.csv")

@st.cache_data
def load_model(which="rf"):
    if which=="xgb":
        return joblib.load(MODELS / "best_model_xgb.joblib")
    return joblib.load(MODELS / "best_model_rf.joblib")

data = load_data()
model_art = load_model(which=st.sidebar.selectbox("Model", ["rf","xgb"]) )
model = model_art["model"]
le = model_art["label_encoder"]
features = model_art["features"]

st.title("AI-EnviroScan ‚Äî Realtime (Synthetic) Dashboard")

with st.sidebar:
    st.header("Controls")
    threshold_pm25 = st.slider("PM2.5 Alert Threshold (¬µg/m¬≥)", 10, 500, 100)
    sample_n = st.slider("Sample points on map", 50, 500, 200)
    st.markdown("### Actions")
    if st.button("Regenerate features & retrain"):
        st.info("Run setup_generate_data_and_train.py externally or rerun modules. (Not done from Streamlit automatically.)")

# KPIs
st.metric("Data points", len(data))
if "pm25" in data.columns:
    st.metric("Avg PM2.5", float(data["pm25"].mean().round(2)))

# Predict
X = data[features].fillna(0)
preds = model.predict(X)
data["pred_label"] = le.inverse_transform(preds)
st.subheader("Predicted Source Distribution")
st.write(data["pred_label"].value_counts())

# Alerts
high_risk = data[data["pm25"] > threshold_pm25]
if not high_risk.empty:
    st.warning(f"{len(high_risk)} locations exceed PM2.5 threshold of {threshold_pm25} ¬µg/m¬≥")
    st.dataframe(high_risk[["location","latitude","longitude","pm25","pred_label"]].head(20))

# Map embed
map_path = MAPS / "dashboard_map.html"
if map_path.exists():
    html = map_path.read_text(encoding="utf-8")
    st.subheader("Heatmap + Sampled Markers")
    components.html(html, height=700, scrolling=True)
else:
    st.info("Map not found. Run src/module5_geospatial_mapping.py to create it.")

st.markdown("---")
st.subheader("Sample data (first 200 rows)")
st.dataframe(data.head(200))

csv = data.to_csv(index=False)
st.download_button("Download processed dataset (CSV)", csv, file_name="features_prelabel_2000.csv", mime="text/csv")

# Alert button that triggers module7
if st.button("Send email/SMS alert now"):
    # Ensure src directory is in sys.path or import correctly
    try:
        import src.module7_alerts as alerts
        alerts.check_and_alert(threshold=threshold_pm25, email_recipient=os.getenv("ALERT_EMAIL_TO"), sms_recipient=os.getenv("ALERT_SMS_TO"))
        st.success("Alert check triggered (logs will show attempts).")
    except ModuleNotFoundError:
        st.error("Could not find module7_alerts.py in the src directory.")
    except Exception as e:
        st.error(f"An error occurred during alert check: {e}")

!pip install streamlit

!streamlit run src/your_app.py --server.port 8501 --server.address 0.0.0.0

!pip install streamlit pyngrok pandas requests folium geopandas osmnx

!ngrok config add-authtoken 32gRbNQaBQj70TVZ7RLRLVYhb1SMP_4BamL47uvBsCk6CrHGvJ7 # Example token

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import requests
# import folium
# from streamlit_folium import st_folium
# import os
# from google.colab import userdata # Import userdata to access secrets
# 
# st.set_page_config(page_title="AQI & Weather Dashboard", layout="wide")
# 
# st.title("üåç Live AQI + Weather Dashboard")
# 
# # --- User input (City and Country) ---
# city = st.text_input("Enter City", "Hyderabad")
# country = st.text_input("Enter Country Code", "IN")
# 
# # --- Get API Key from Secrets ---
# # Make sure you have added your OpenWeatherMap API key to Colab Secrets
# # with the name 'OPENWEATHER_API_KEY'.
# OWM_API_KEY = userdata.get('https://api.openaq.org')
# 
# if st.button("Fetch Data"):
#     # ------------------------------
#     # 1. AQI Data (OpenAQ)
#     # ------------------------------
#     # NOTE: The OpenAQ API endpoint used below returned a "410 Client Error: Gone" in previous runs.
#     # This means the endpoint is no longer available. You will need to find and update this URL
#     # with a valid OpenAQ API endpoint (or an alternative data source) to collect live data.
#     openaq_url = "https://api.openaq.org" # This URL might be outdated
#     try:
#         params = {"city": city, "country": country, "limit": 10}
#         # This request will likely fail with a 410 Gone error
#         r = requests.get(openaq_url, params=params, timeout=20)
#         r.raise_for_status()
#         data = r.json()
#         records = []
#         for loc in data.get("results", []):
#             coords = loc.get("coordinates", {})
#             for meas in loc.get("measurements", []):
#                 records.append({
#                     "location": loc.get("location"),
#                     "parameter": meas.get("parameter"),
#                     "value": meas.get("value"),
#                     "unit": meas.get("unit"),
#                     "latitude": coords.get("latitude"),
#                     "longitude": coords.get("longitude")
#                 })
#         aqi_df = pd.DataFrame(records)
#         st.subheader("üìä Air Quality Data")
#         st.write(aqi_df)
#     except Exception as e:
#         st.error(f"Error fetching AQI data: {e}")
#         aqi_df = pd.DataFrame()
# 
#     # ------------------------------
#     # 2. Weather Data (OpenWeather)
#     # ------------------------------
#     try:
#         if OWM_API_KEY:
#             weather_url = "https://api.openweathermap.org/data/2.5/"
#             params = {"q": city, "appid": OWM_API_KEY, "units": "metric"} # Use OWM_API_KEY from secrets
#             w = requests.get(weather_url, params=params, timeout=20)
#             w.raise_for_status()
#             wdata = w.json()
#             weather_info = {
#                 "Temperature (¬∞C)": wdata["main"]["temp"],
#                 "Humidity (%)": wdata["main"]["humidity"],
#                 "Wind Speed (m/s)": wdata["wind"]["speed"],
#                 "Condition": wdata["weather"][0]["description"],
#             }
#             st.subheader("üå¶Ô∏è Weather Data")
#             st.write(weather_info)
#         else:
#             st.warning("OpenWeatherMap API Key not found in Colab Secrets. Please add it with the name 'OPENWEATHER_API_KEY'.")
#     except Exception as e:
#         st.error(f"Error fetching Weather data: {e}")
# 
#     # ------------------------------
#     # 3. Map Visualization
#     # ------------------------------
#     if not aqi_df.empty:
#         st.subheader("üó∫Ô∏è Map Visualization")
#         # Calculate mean coordinates only if aqi_df is not empty
#         center_lat = aqi_df.latitude.mean() if not aqi_df.empty else 0
#         center_lon = aqi_df.longitude.mean() if not aqi_df.empty else 0
#         m = folium.Map(location=[center_lat, center_lon], zoom_start=10)
#         for _, row in aqi_df.iterrows():
#             # Ensure lat/lon are not None before creating marker
#             if pd.notna(row["latitude"]) and pd.notna(row["longitude"]):
#                 folium.CircleMarker(
#                     location=(row["latitude"], row["longitude"]),
#                     radius=7,
#                     color="red",
#                     fill=True,
#                     fill_color="red",
#                     popup=f"{row['parameter']}: {row['value']} {row['unit']}"
#                 ).add_to(m)
#         st_folium(m, width=700, height=500)
#     elif not aqi_df.empty: # This condition will only be met if aqi_df is NOT empty
#          st.info("No AQI data with valid coordinates to display on the map.")
# 
# 
# st.markdown("---")
# st.info("‚úÖ Enter city and country code, add API keys to Colab Secrets, then click 'Fetch Data'.")

!ls src

!streamlit run your_app.py &>/dev/null &

!pip install pyngrok

from pyngrok import ngrok

# Kill any old tunnels
ngrok.kill()

public_url = ngrok.connect(8501, "http")
print("üåç Streamlit public URL:", public_url)

from pyngrok import ngrok

# Kill any existing tunnels
ngrok.kill()

# Open tunnel for port 8501
public_url = ngrok.connect(8501)
print("Streamlit public URL:", public_url)

# 5Ô∏è‚É£ Run Streamlit in the background
# Use & to run in background so ngrok can forward traffic
!nohup streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &

# 6Ô∏è‚É£ Reopen tunnel (after Streamlit has started)
from pyngrok import ngrok
public_url = ngrok.connect(8501)
print("üîó Streamlit app available at:", public_url)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import requests
# import folium
# from streamlit_folium import st_folium
# 
# st.set_page_config(page_title="AQI & Weather Dashboard", layout="wide")
# st.title("üåç Live AQI + Weather Dashboard")
# 
# city = st.text_input("Enter City", "Hyderabad")
# country = st.text_input("Enter Country Code", "IN")
# owm_api = st.text_input("Enter OpenWeatherMap API Key", type="password")
# 
# if st.button("Fetch Data"):
#     try:
#         openaq_url = "https://api.openaq.org/v2/latest"
#         params = {"city": city, "country": country, "limit": 10}
#         r = requests.get(openaq_url, params=params, timeout=20)
#         r.raise_for_status()
#         data = r.json()
#         records = []
#         for loc in data.get("results", []):
#             coords = loc.get("coordinates", {})
#             for meas in loc.get("measurements", []):
#                 records.append({
#                     "location": loc.get("location"),
#                     "parameter": meas.get("parameter"),
#                     "value": meas.get("value"),
#                     "unit": meas.get("unit"),
#                     "latitude": coords.get("latitude"),
#                     "longitude": coords.get("longitude")
#                 })
#         aqi_df = pd.DataFrame(records)
#         st.subheader("üìä Air Quality Data")
#         st.write(aqi_df)
#     except Exception as e:
#         st.error(f"Error fetching AQI data: {e}")
#         aqi_df = pd.DataFrame()
#

!pip install --quiet streamlit pyngrok pandas requests folium geopandas osmnx streamlit-folium

from pyngrok import ngrok

# Set your ngrok token
NGROK_AUTHTOKEN = "33CVXCwyFxjPDTYAydhXr99ot3f_yuwti2RtXDB7DYTVQsqP"
ngrok.set_auth_token(NGROK_AUTHTOKEN)

# Kill previous tunnels (if any)
ngrok.kill()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import requests
# import folium
# from streamlit_folium import st_folium
# 
# st.set_page_config(page_title="AQI + Weather Dashboard", layout="wide")
# st.title("üåç Live AQI + Weather Dashboard")
# 
# # User input
# city = st.text_input("Enter City", "Hyderabad")
# country = st.text_input("Enter Country Code", "IN")
# owm_api = st.text_input("Enter OpenWeatherMap API Key", type="password")
# 
# if st.button("Fetch Data"):
#     # ------------------- AQI Data -------------------
#     try:
#         openaq_url = "https://api.openaq.org/v2/measurements"
#         params = {
#             "country": country,
#             "limit": 50,
#             "sort": "desc",
#             "order_by": "datetime"
#         }
#         r = requests.get(openaq_url, params=params, timeout=20)
#         r.raise_for_status()
#         data = r.json()
#         records = []
#         for meas in data.get("results", []):
#             coords = meas.get("coordinates", {})
#             records.append({
#                 "location": meas.get("location"),
#                 "parameter": meas.get("parameter"),
#                 "value": meas.get("value"),
#                 "unit": meas.get("unit"),
#                 "latitude": coords.get("latitude"),
#                 "longitude": coords.get("longitude")
#             })
#         aqi_df = pd.DataFrame(records)
#         # Filter by city if coordinates exist
#         if not aqi_df.empty and city:
#             aqi_df = aqi_df[aqi_df['location'].str.contains(city, case=False)]
#         st.subheader("üìä Air Quality Data")
#         st.write(aqi_df)
#     except Exception as e:
#         st.error(f"Error fetching AQI data: {e}")
#         aqi_df = pd.DataFrame()
# 
#     # ------------------- Weather Data -------------------
#     try:
#         if owm_api:
#             weather_url = "http://api.openweathermap.org/data/2.5/weather"
#             params = {"q": city, "appid": owm_api, "units": "metric"}
#             w = requests.get(weather_url, params=params, timeout=20)
#             w.raise_for_status()
#             wdata = w.json()
#             weather_info = {
#                 "Temperature (¬∞C)": wdata["main"]["temp"],
#                 "Humidity (%)": wdata["main"]["humidity"],
#                 "Wind Speed (m/s)": wdata["wind"]["speed"],
#                 "Condition": wdata["weather"][0]["description"],
#             }
#             st.subheader("üå¶Ô∏è Weather Data")
#             st.write(weather_info)
#         else:
#             st.warning("Please enter your OpenWeatherMap API key.")
#     except Exception as e:
#         st.error(f"Error fetching Weather data: {e}")
# 
#     # ------------------- Map -------------------
#     if not aqi_df.empty:
#         st.subheader("üó∫Ô∏è Map Visualization")
#         m = folium.Map(location=[aqi_df.latitude.mean(), aqi_df.longitude.mean()], zoom_start=10)
#         for _, row in aqi_df.iterrows():
#             folium.CircleMarker(
#                 location=(row["latitude"], row["longitude"]),
#                 radius=7,
#                 color="red",
#                 fill=True,
#                 fill_color="red",
#                 popup=f"{row['parameter']}: {row['value']} {row['unit']}"
#             ).add_to(m)
#         st_folium(m, width=700, height=500)
# 
# st.info("‚úÖ Enter city, country code, and your API key, then click 'Fetch Data'.")
#

# Run Streamlit app in background
!nohup streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &

public_url = ngrok.connect(8501)
print("üîó Streamlit app available at:", public_url)

!pip install --quiet streamlit pyngrok pandas requests folium streamlit-folium

from pyngrok import ngrok

# Set your ngrok token
NGROK_AUTHTOKEN = "33CVXCwyFxjPDTYAydhXr99ot3f_yuwti2RtXDB7DYTVQsqP"
ngrok.set_auth_token(NGROK_AUTHTOKEN)
ngrok.kill()  # Close previous tunnels if any

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import requests
# import folium
# from streamlit_folium import st_folium
# 
# st.set_page_config(page_title="AQI + Weather Dashboard", layout="wide")
# st.title("üåç Live AQI + Weather Dashboard")
# 
# city = st.text_input("Enter City", "Hyderabad")
# country = st.text_input("Enter Country Code", "IN")
# owm_api = st.text_input("Enter OpenWeatherMap API Key", type="password")
# 
# if st.button("Fetch Data"):
# 
#     # --------- AQI Data (OpenAQ v3) ---------
#     try:
#         openaq_url = "https://api.openaq.org/v3/measurements"
#         params = {
#             "country": country,
#             "limit": 100,
#             "sort": "desc"
#         }
#         r = requests.get(openaq_url, params=params, timeout=20)
#         r.raise_for_status()
#         data = r.json()
#         records = []
#         for meas in data.get("results", []):
#             coords = meas.get("coordinates")
#             if coords:
#                 records.append({
#                     "location": meas.get("location"),
#                     "parameter": meas.get("parameter"),
#                     "value": meas.get("value"),
#                     "unit": meas.get("unit"),
#                     "latitude": coords.get("latitude"),
#                     "longitude": coords.get("longitude")
#                 })
#         aqi_df = pd.DataFrame(records)
# 
#         # Filter by city if possible
#         if not aqi_df.empty and city:
#             aqi_df = aqi_df[aqi_df['location'].str.contains(city, case=False)]
# 
#         st.subheader("üìä Air Quality Data")
#         st.write(aqi_df)
#     except Exception as e:
#         st.error(f"Error fetching AQI data: {e}")
#         aqi_df = pd.DataFrame()
# 
#     # --------- Weather Data ---------
#     try:
#         if owm_api:
#             weather_url = "https://api.openweathermap.org/data/2.5/weather"
#             params = {"q": city, "appid": owm_api, "units": "metric"}
#             w = requests.get(weather_url, params=params, timeout=20)
#             w.raise_for_status()
#             wdata = w.json()
#             weather_info = {
#                 "Temperature (¬∞C)": wdata["main"]["temp"],
#                 "Humidity (%)": wdata["main"]["humidity"],
#                 "Wind Speed (m/s)": wdata["wind"]["speed"],
#                 "Condition": wdata["weather"][0]["descriptio]()
#

!nohup streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &

public_url = ngrok.connect(8501)
print("üîó Streamlit app available at:", public_url)

!pip install --quiet streamlit pyngrok pandas requests folium geopandas osmnx streamlit-folium shapely

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import requests
# import folium
# import osmnx as ox
# import geopandas as gpd
# from shapely.geometry import Point
# from streamlit_folium import st_folium
# 
# st.set_page_config(page_title="AQI + Pollution Source Dashboard", layout="wide")
# st.title("üåç Live AQI + Weather + Pollution Dashboard")
# 
# city = st.text_input("Enter City", "Hyderabad")
# country = st.text_input("Enter Country Code", "IN")
# owm_api = st.text_input("Enter OpenWeatherMap API Key", type="password")
# 
# if st.button("Fetch Data"):
# 
#     # --------- AQI Data (OpenAQ v3) ---------
#     try:
#         openaq_url = "https://api.openaq.org/v3/measurements"
#         params = {"country": country, "limit": 50, "sort": "desc"}
#         r = requests.get(openaq_url, params=params, timeout=20)
#         r.raise_for_status()
#         data = r.json()
#         records = []
#         for meas in data.get("results", []):
#             coords = meas.get("coordinates")
#             if coords:
#                 records.append({
#                     "location": meas.get("location"),
#                     "parameter": meas.get("parameter"),
#                     "value": meas.get("value"),
#                     "unit": meas.get("unit"),
#                     "latitude": coords.get("latitude"),
#                     "longitude": coords.get("longitude")
#                 })
#         aqi_df = pd.DataFrame(records)
#         if not aqi_df.empty and city:
#             aqi_df = aqi_df[aqi_df['location'].str.contains(city, case=False)]
#         st.subheader("üìä Air Quality Data")
#         st.write(aqi_df)
#     except Exception as e:
#         st.error(f"Error fetching AQI data: {e}")
#         aqi_df = pd.DataFrame()
# 
#     # --------- Weather Data ---------
#     try:
#         if owm_api:
#             weather_url = "https://api.openweathermap.org/data/2.5/"
#             params = {"q": city, "appid": owm_api, "units": "metric"}
#             w = requests.get(weather_url, params=params, timeout=20)
#             w.raise_for_status()
#             wdata = w.json()
#             weather_info = {
#                 "Temperature (¬∞C)": wdata["main"]["temp"],
#                 "Humidity (%)": wdata["main"]["humidity"],
#                 "Wind Speed (m/s)": wdata["wind"]["speed"],
#                 "Condition": wdata["weather"][0]["description"]
#             }
#             st.subheader("üå¶Ô∏è Weather Data")
#             st.write(weather_info)
#         else:
#             st.warning("Please enter a valid OpenWeatherMap API key.")
#     except Exception as e:
#         st.error(f"Error fetching Weather data: {e}")
# 
#     # --------- Compute Distances to Roads / Industry ---------
#     if not aqi_df.empty:
#         st.subheader("üó∫Ô∏è Map with Roads and Industrial Areas")
#         center_lat = aqi_df.latitude.mean()
#         center_lon = aqi_df.longitude.mean()
# 
#         gdf_points = gpd.GeoDataFrame(aqi_df,
#                                       geometry=gpd.points_from_xy(aqi_df.longitude, aqi_df.latitude),
#                                       crs="EPSG:4326").to_crs(epsg=3857)
# 
#         # Roads
#         roads = ox.geometries_from_point((center_lat, center_lon), tags={"highway": True}, dist=3000)
#         if not roads.empty: roads = roads.to_crs(epsg=3857)
# 
#         # Industrial areas
#         industry = ox.geometries_from_point((center_lat, center_lon), tags={"landuse":"industrial"}, dist=3000)
#         if not industry.empty: industry = industry.to_crs(epsg=3857)
# 
#         def nearest_distance(point, gdf):
#             if gdf.empty: return float("nan")
#             return gdf.geometry.distance(point).min()
# 
#         gdf_points["dist_to_road_m"] = gdf_points.geometry.apply(lambda p: nearest_distance(p, roads))
#         gdf_points["dist_to_industry_m"] = gdf_points.geometry.apply(lambda p: nearest_distance(p, industry))
# 
#         st.write(gdf_points[["location","parameter","value","dist_to_road_m","dist_to_industry_m"]])
# 
#         # Map
#         m = folium.Map(location=[center_lat, center_lon], zoom_start=12)
# 
#         for _, row in gdf_points.iterrows():
#             color = "green" if row["dist_to_industry_m"] > 1000 else "orange"
#             folium.CircleMarker(
#                 location=(row.geometry.y, row.geometry.x),
#                 radius=7,
#                 color=color,
#                 fill=True,
#                 fill_color=color,
#                 popup=f"{row['parameter']}: {row['value']} {row['unit']}\nDist to Road: {row['dist_to_road_m']:.1f}m\nDist to Industry: {row['dist_to_industry_m']:.1f}m"
#             ).add_to(m)
# 
#         st_folium(m, width=700, height=500)
#

!nohup streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &

from pyngrok import ngrok
public_url = ngrok.connect(8501)
print("üîó Streamlit app available at:", public_url)

import os
print(os.getcwd())

!pip install requests

!npx localtunnel --port 8501

# In Colab, you can set environment variables like this
import os

os.environ["OPENWEATHER_API_KEY"] = "47d01273f50c456dc3e575754dd3d7d2"

import requests
import os

def get_weather(city):
    api_key = os.getenv("OPENWEATHER_API_KEY")
    if not api_key:
        return None
    url = f"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        weather = {
            "Temperature (¬∞C)": data["main"]["temp"],
            "Humidity (%)": data["main"]["humidity"],
            "Wind Speed (m/s)": data["wind"]["speed"],
            "Condition": data["weather"][0]["description"]
        }
        return weather
    except Exception as e:
        return {"Error": str(e)}

# Install Streamlit, pyngrok for Colab, and requests for API calls
!pip install streamlit pyngrok requests pandas

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import requests
# import os
# from datetime import datetime
# 
# # ----------------------
# # Sidebar Inputs
# # ----------------------
# st.sidebar.title("Pollution Dashboard")
# city = st.sidebar.text_input("Enter City", "Hyderabad")
# 
# # ----------------------
# # Function to fetch real-time weather
# # ----------------------
# def get_weather(city):
#     api_key = os.getenv("OPENWEATHER_API_KEY")  # Set your API key
#     if not api_key:
#         return {"Error": "Missing API Key"}
#     url = f"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
#     try:
#         resp = requests.get(url, timeout=10)
#         resp.raise_for_status()
#         data = resp.json()
#         weather = {
#             "Temperature (¬∞C)": data["main"]["temp"],
#             "Humidity (%)": data["main"]["humidity"],
#             "Wind Speed (m/s)": data["wind"]["speed"],
#             "Condition": data["weather"][0]["description"]
#         }
#         return weather
#     except Exception as e:
#         return {"Error": str(e)}
# 
# # ----------------------
# # Display Weather Info
# # ----------------------
# st.subheader(f"üå¶Ô∏è Current Weather in {city}")
# weather = get_weather(city)
# st.json(weather)
# 
# # ----------------------
# # Load Pollution Prediction Data
# # ----------------------
# # Example: Using pre-generated CSV predictions
# try:
#     df_pollution = pd.read_csv("source_prediction_outputs.csv")
#     df_pollution['timestamp'] = pd.to_datetime(df_pollution['timestamp'])
# except:
#     st.warning("Pollution prediction dataset not found.")
#     df_pollution = pd.DataFrame()
# 
# # ----------------------
# # Filter by City
# # ----------------------
# if not df_pollution.empty:
#     df_city = df_pollution[df_pollution['city'] == city]
#     st.subheader(f"üü¢ Pollution Predictions in {city}")
#     st.dataframe(df_city[['timestamp','pm25','pm10','no2','so2','co','o3','predicted_label']])
# 
#     # Map source labels to human-readable categories
#     df_city['Source'] = df_city['predicted_label'].map({0:'Industrial',1:'Vehicular'})
# 
#     # Display Source Distribution
#     st.subheader("üìä Predicted Source Distribution")
#     st.bar_chart(df_city['Source'].value_counts())
# 
#

!pip install streamlit folium streamlit-folium pandas numpy

import folium
from folium.plugins import HeatMap

# Function to generate the map
def generate_map(df_filtered):
    # Center the map on the mean coordinates
    center_lat = df_filtered['latitude'].mean()
    center_lon = df_filtered['longitude'].mean()
    m = folium.Map(location=[center_lat, center_lon], zoom_start=6)

    # Add HeatMap for PM2.5 severity
    heat_data = [[row['latitude'], row['longitude'], row['pm25']] for index, row in df_filtered.iterrows()]
    HeatMap(heat_data, radius=15).add_to(m)

    # Add markers for source category
    for _, row in df_filtered.iterrows():
        color = 'red' if row['source_category']=='Industrial' else 'blue'
        folium.CircleMarker(
            location=[row['latitude'], row['longitude']],
            radius=6,
            color=color,
            fill=True,
            fill_color=color,
            popup=f"{row['city']} | {row['source_category']} | PM2.5: {row['pm25']:.1f}"
        ).add_to(m)

    return m

import streamlit as st
import requests
import pandas as pd
from io import BytesIO

# -------------------------------
# Title
# -------------------------------
st.title("üåç Live AQI + Weather Dashboard")

# -------------------------------
# Inputs
# -------------------------------
city = st.text_input("Enter City", "Hyderabad")
country = st.text_input("Enter Country Code", "IN")
api_key = st.text_input("Enter OpenWeatherMap API Key", type="password")

# -------------------------------
# Fetch Data Functions
# -------------------------------
def get_weather(city, country, api_key):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city},{country}&appid={api_key}&units=metric"
    response = requests.get(url).json()
    if response.get("cod") != 200:
        return None
    return {
        "Temperature (¬∞C)": response["main"]["temp"],
        "Humidity (%)": response["main"]["humidity"],
        "Wind Speed (m/s)": response["wind"]["speed"],
        "Weather": response["weather"][0]["description"]
    }

def get_air_quality(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
    response = requests.get(url).json()
    if "list" not in response:
        return None
    components = response["list"][0]["components"]
    return {
        "PM2.5": components["pm2_5"],
        "PM10": components["pm10"],
        "NO‚ÇÇ": components["no2"],
        "SO‚ÇÇ": components["so2"],
        "CO": components["co"],
        "O‚ÇÉ": components["o3"]
    }

# -------------------------------
# Main Logic
# -------------------------------
if api_key:
    weather_data = get_weather(city, country, api_key)

    if weather_data:
        st.subheader("üå§Ô∏è Weather Data")
        st.write(weather_data)

        # Get coordinates for AQI
        url_geo = f"http://api.openweathermap.org/geo/1.0/direct?q={city},{country}&limit=1&appid={api_key}"
        geo = requests.get(url_geo).json()
        if geo:
            lat, lon = geo[0]["lat"], geo[0]["lon"]
            aqi_data = get_air_quality(lat, lon, api_key)

            if aqi_data:
                st.subheader("üí® Air Quality Data")
                st.write(aqi_data)

                # Combine both datasets
                combined_data = {**weather_data, **aqi_data}
                df = pd.DataFrame([combined_data])

                # Show table
                st.dataframe(df)

                # Download button
                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• Download Data as CSV",
                    data=csv,
                    file_name=f"{city}_AQI_Weather.csv",
                    mime="text/csv"
                )

            else:
                st.error("Could not fetch Air Quality data ‚ùå")
        else:
            st.error("Could not fetch location details ‚ùå")
    else:
        st.error("Invalid City or API Key ‚ùå")

pip install streamlit requests pandas altair pydeck

import requests

API_KEY = "47d01273f50c456dc3e575754dd3d7d2"
city, country = "Hyderabad", "IN"

# Geocoding
geo_url = f"http://api.openweathermap.org/geo/1.0/direct?q={city},{country}&limit=1&appid={API_KEY}"
print(requests.get(geo_url).json())

# Weather
lat, lon = 17.385, 78.4867
weather_url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={API_KEY}&units=metric"
print(requests.get(weather_url).json())

# Air Pollution
pollution_url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={API_KEY}"
print(requests.get(pollution_url).json())

!pip install streamlit pyngrok requests pandas altair pydeck

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# # paste the full code I gave earlier here
#

!streamlit run app.py &>/dev/null&

from pyngrok import ngrok
public_url = ngrok.connect(8501)
print("Dashboard URL:", public_url)

import streamlit as st
import requests
import pandas as pd
import datetime as dt
import os

# -----------------------------
# Setup
# -----------------------------
st.set_page_config(page_title="India AQI + Weather Dashboard", layout="wide")

DATA_DIR = "india_city_data"
os.makedirs(DATA_DIR, exist_ok=True)

# List of Indian cities (expandable)
INDIAN_CITIES = [
    "Hyderabad", "Delhi", "Mumbai", "Chennai", "Bengaluru",
    "Kolkata", "Pune", "Ahmedabad", "Jaipur", "Lucknow"
]

DEFAULT_COUNTRY = "IN"

# -----------------------------
# Helper functions
# -----------------------------
def get_coords(city, api_key):
    url = f"http://api.openweathermap.org/geo/1.0/direct?q={city},{DEFAULT_COUNTRY}&limit=1&appid={api_key}"
    r = requests.get(url)
    if r.status_code == 200 and r.json():
        return r.json()[0]["lat"], r.json()[0]["lon"]
    return None, None

def get_weather(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric"
    r = requests.get(url)
    if r.status_code == 200:
        j = r.json()
        return {
            "temperature_C": j["main"]["temp"],
            "humidity_pct": j["main"]["humidity"],
            "wind_speed_m_s": j["wind"]["speed"],
            "weather_desc": j["weather"][0]["description"]
        }
    return None

def get_pollution(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
    r = requests.get(url)
    if r.status_code == 200:
        j = r.json()
        if "list" in j and j["list"]:
            comp = j["list"][0]["components"]
            return {
                "pm2_5": comp.get("pm2_5"),
                "pm10": comp.get("pm10"),
                "no2": comp.get("no2"),
                "so2": comp.get("so2"),
                "co": comp.get("co"),
                "o3": comp.get("o3"),
                "aqi_index": j["list"][0]["main"]["aqi"]
            }
    return None

def save_data(city, row):
    file_path = os.path.join(DATA_DIR, f"{city}_data.csv")
    df = pd.DataFrame([row])
    if not os.path.exists(file_path):
        df.to_csv(file_path, index=False)
    else:
        df.to_csv(file_path, mode="a", header=False, index=False)

def load_data(city):
    file_path = os.path.join(DATA_DIR, f"{city}_data.csv")
    if os.path.exists(file_path):
        return pd.read_csv(file_path, parse_dates=["timestamp"])
    return pd.DataFrame()

# -----------------------------
# Streamlit UI
# -----------------------------
st.title("üåç India AQI + Weather Dashboard")

api_key = st.text_input("Enter your OpenWeatherMap API Key", type="password")
city = st.selectbox("Select a City in India", INDIAN_CITIES)

if st.button("üîÑ Fetch Current Data"):
    if not api_key:
        st.error("Please enter your OpenWeatherMap API Key.")
    else:
        lat, lon = get_coords(city, api_key)
        if not lat:
            st.error("Failed to fetch coordinates. Check API key/city name.")
        else:
            weather = get_weather(lat, lon, api_key)
            pollution = get_pollution(lat, lon, api_key)

            if weather and pollution:
                timestamp = dt.datetime.utcnow()
                row = {
                    "timestamp": timestamp,
                    "city": city,
                    "lat": lat,
                    "lon": lon,
                    **weather,
                    **pollution
                }
                save_data(city, row)
                st.success(f"Data fetched for {city}")
                st.table(pd.DataFrame([row]).T.rename(columns={0:"value"}))
            else:
                st.error("Failed to fetch weather or pollution data.")

# -----------------------------
# Data Display + Download
# -----------------------------
df = load_data(city)

if not df.empty:
    st.subheader(f"Stored Data for {city}")
    st.dataframe(df.tail(10))

    # Download buttons
    csv_all = df.to_csv(index=False).encode("utf-8")
    st.download_button("üì• Download All Data (CSV)", csv_all, f"{city}_all_data.csv", "text/csv")

    # Daily aggregated report
    df["date"] = df["timestamp"].dt.date
    daily = df.groupby("date").mean(numeric_only=True).reset_index()
    csv_daily = daily.to_csv(index=False).encode("utf-8")
    st.download_button("üì• Download Daily Report (CSV)", csv_daily, f"{city}_daily_report.csv", "text/csv")

    # Weekly aggregated report
    df["week"] = df["timestamp"].dt.to_period("W").apply(lambda r: r.start_time)
    weekly = df.groupby("week").mean(numeric_only=True).reset_index()
    csv_weekly = weekly.to_csv(index=False).encode("utf-8")
    st.download_button("üì• Download Weekly Report (CSV)", csv_weekly, f"{city}_weekly_report.csv", "text/csv")
else:
    st.info(f"No stored data for {city}. Click 'Fetch Current Data' first.")

import streamlit as st
import requests
import pandas as pd
import datetime as dt
import os
import plotly.express as px
import folium
from streamlit_folium import st_folium
from folium.plugins import HeatMap

# -----------------------------
# Setup
# -----------------------------
st.set_page_config(page_title="India AQI + Weather Dashboard", layout="wide")

DATA_DIR = "india_city_data"
os.makedirs(DATA_DIR, exist_ok=True)

# Indian Cities
INDIAN_CITIES = [
    "Hyderabad", "Delhi", "Mumbai", "Chennai", "Bengaluru",
    "Kolkata", "Pune", "Ahmedabad", "Jaipur", "Lucknow"
]

DEFAULT_COUNTRY = "IN"

# -----------------------------
# Helper Functions
# -----------------------------
def get_coords(city, api_key):
    url = f"http://api.openweathermap.org/geo/1.0/direct?q={city},{DEFAULT_COUNTRY}&limit=1&appid={api_key}"
    r = requests.get(url)
    if r.status_code == 200 and r.json():
        return r.json()[0]["lat"], r.json()[0]["lon"]
    return None, None

def get_weather(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric"
    r = requests.get(url)
    if r.status_code == 200:
        j = r.json()
        return {
            "temperature_C": j["main"]["temp"],
            "humidity_pct": j["main"]["humidity"],
            "wind_speed_m_s": j["wind"]["speed"],
            "weather_desc": j["weather"][0]["description"].title()
        }
    return None

def get_pollution(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
    r = requests.get(url)
    if r.status_code == 200:
        j = r.json()
        if "list" in j and j["list"]:
            comp = j["list"][0]["components"]
            return {
                "pm2_5": comp.get("pm2_5"),
                "pm10": comp.get("pm10"),
                "no2": comp.get("no2"),
                "so2": comp.get("so2"),
                "co": comp.get("co"),
                "o3": comp.get("o3"),
                "aqi_index": j["list"][0]["main"]["aqi"]
            }
    return None

def save_data(city, row):
    file_path = os.path.join(DATA_DIR, f"{city}_data.csv")
    df = pd.DataFrame([row])
    if not os.path.exists(file_path):
        df.to_csv(file_path, index=False)
    else:
        df.to_csv(file_path, mode="a", header=False, index=False)

def load_data(city):
    file_path = os.path.join(DATA_DIR, f"{city}_data.csv")
    if os.path.exists(file_path):
        return pd.read_csv(file_path, parse_dates=["timestamp"])
    return pd.DataFrame()

# -----------------------------
# Sidebar (Inputs)
# -----------------------------
st.sidebar.header("‚öôÔ∏è Dashboard Settings")
api_key = st.sidebar.text_input("üîë OpenWeatherMap API Key", type="password")
city = st.sidebar.selectbox("üèôÔ∏è Select City", INDIAN_CITIES)
fetch = st.sidebar.button("üîÑ Fetch Current Data")

# -----------------------------
# Main Dashboard
# -----------------------------
st.title("üåç India AQI + Weather Dashboard with Maps")
st.caption("üì° Real-time Weather, Pollution Data, Heatmaps & Air Maps")

if fetch:
    if not api_key:
        st.error("Please enter your OpenWeatherMap API Key.")
    else:
        lat, lon = get_coords(city, api_key)
        if not lat:
            st.error("Failed to fetch coordinates. Check API key/city name.")
        else:
            weather = get_weather(lat, lon, api_key)
            pollution = get_pollution(lat, lon, api_key)

            if weather and pollution:
                timestamp = dt.datetime.utcnow()
                row = {
                    "timestamp": timestamp,
                    "city": city,
                    "lat": lat,
                    "lon": lon,
                    **weather,
                    **pollution
                }
                save_data(city, row)
                st.success(f"‚úÖ Data fetched for {city} at {timestamp.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            else:
                st.error("Failed to fetch weather or pollution data.")

# -----------------------------
# Display Stored Data
# -----------------------------
df = load_data(city)

if not df.empty:
    # Latest Data Cards
    st.subheader(f"üìä Latest Data for {city}")
    latest = df.iloc[-1]

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("üå°Ô∏è Temp (¬∞C)", f"{latest['temperature_C']:.1f}")
    col2.metric("üíß Humidity (%)", f"{latest['humidity_pct']}")
    col3.metric("üå¨Ô∏è Wind (m/s)", f"{latest['wind_speed_m_s']}")
    col4.metric("üõë AQI Index", f"{latest['aqi_index']}")

    # Charts
    st.subheader("üìà Trends Over Time")
    col1, col2 = st.columns(2)
    with col1:
        fig1 = px.line(df, x="timestamp", y=["pm2_5", "pm10"], title="PM2.5 & PM10 Levels")
        st.plotly_chart(fig1, use_container_width=True)
    with col2:
        fig2 = px.line(df, x="timestamp", y=["no2", "so2", "co", "o3"], title="Other Pollutants")
        st.plotly_chart(fig2, use_container_width=True)

    # Heatmap
    st.subheader("üî• AQI Heatmap")
    m = folium.Map(location=[latest["lat"], latest["lon"]], zoom_start=6)
    heat_data = [[row["lat"], row["lon"], row["pm2_5"]] for _, row in df.iterrows() if not pd.isna(row["pm2_5"])]
    if heat_data:
        HeatMap(heat_data, radius=20).add_to(m)
    st_folium(m, width=700, height=500)

    # Weather Air Map
    st.subheader("üå¨Ô∏è Weather & Air Quality Map")
    m2 = folium.Map(location=[latest["lat"], latest["lon"]], zoom_start=6)
    folium.Marker(
        [latest["lat"], latest["lon"]],
        popup=f"{city}: {latest['weather_desc']} | Temp: {latest['temperature_C']}¬∞C | AQI: {latest['aqi_index']}",
        tooltip=city,
        icon=folium.Icon(color="red" if latest['aqi_index'] > 3 else "green")
    ).add_to(m2)
    st_folium(m2, width=700, height=500)

    # Data Table
    st.subheader("üìã Recent Records")
    st.dataframe(df.tail(10))

    # Downloads
    st.subheader("üì• Download Reports")
    csv_all = df.to_csv(index=False).encode("utf-8")
    st.download_button("‚¨áÔ∏è Download All Data (CSV)", csv_all, f"{city}_all_data.csv", "text/csv")

    df["date"] = df["timestamp"].dt.date
    daily = df.groupby("date").mean(numeric_only=True).reset_index()
    csv_daily = daily.to_csv(index=False).encode("utf-8")
    st.download_button("‚¨áÔ∏è Download Daily Report (CSV)", csv_daily, f"{city}_daily_report.csv", "text/csv")

    df["week"] = df["timestamp"].dt.to_period("W").apply(lambda r: r.start_time)
    weekly = df.groupby("week").mean(numeric_only=True).reset_index()
    csv_weekly = weekly.to_csv(index=False).encode("utf-8")
    st.download_button("‚¨áÔ∏è Download Weekly Report (CSV)", csv_weekly, f"{city}_weekly_report.csv", "text/csv")
else:
    st.info(f"No stored data for {city}. Use sidebar to fetch latest readings.")

!streamlit run app.py &>/dev/null&
from pyngrok import ngrok
ngrok.connect(8501)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import requests
# import pandas as pd
# import datetime as dt
# import os
# import plotly.express as px
# import folium
# from streamlit_folium import st_folium
# from folium.plugins import HeatMap
# 
# st.set_page_config(page_title="India Multi-City AQI Dashboard", layout="wide")
# DATA_DIR = "india_city_data"
# os.makedirs(DATA_DIR, exist_ok=True)
# 
# INDIAN_CITIES = [
#     "Hyderabad", "Delhi", "Mumbai", "Chennai", "Bengaluru",
#     "Kolkata", "Pune", "Ahmedabad", "Jaipur", "Lucknow"
# ]
# DEFAULT_COUNTRY = "IN"
# 
# def get_coords(city, api_key):
#     url = f"http://api.openweathermap.org/geo/1.0/direct?q={city},{DEFAULT_COUNTRY}&limit=1&appid={api_key}"
#     r = requests.get(url)
#     if r.status_code == 200 and r.json():
#         return r.json()[0]["lat"], r.json()[0]["lon"]
#     return None, None
# 
# def get_weather(lat, lon, api_key):
#     url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric"
#     r = requests.get(url)
#     if r.status_code == 200:
#         j = r.json()
#         return {
#             "temperature_C": j["main"]["temp"],
#             "humidity_pct": j["main"]["humidity"],
#             "wind_speed_m_s": j["wind"]["speed"],
#             "weather_desc": j["weather"][0]["description"].title()
#         }
#     return None
# 
# def get_pollution(lat, lon, api_key):
#     url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
#     r = requests.get(url)
#     if r.status_code == 200:
#         j = r.json()
#         if "list" in j and j["list"]:
#             comp = j["list"][0]["components"]
#             return {
#                 "pm2_5": comp.get("pm2_5"),
#                 "pm10": comp.get("pm10"),
#                 "no2": comp.get("no2"),
#                 "so2": comp.get("so2"),
#                 "co": comp.get("co"),
#                 "o3": comp.get("o3"),
#                 "aqi_index": j["list"][0]["main"]["aqi"]
#             }
#     return None
# 
# def save_data(city, row):
#     file_path = os.path.join(DATA_DIR, f"{city}_data.csv")
#     df = pd.DataFrame([row])
#     if not os.path.exists(file_path):
#         df.to_csv(file_path, index=False)
#     else:
#         df.to_csv(file_path, mode="a", header=False, index=False)
# 
# def load_data(city):
#     file_path = os.path.join(DATA_DIR, f"{city}_data.csv")
#     if os.path.exists(file_path):
#         return pd.read_csv(file_path, parse_dates=["timestamp"])
#     return pd.DataFrame()
# 
# # Sidebar
# st.sidebar.header("‚öôÔ∏è Dashboard Settings")
# api_key = st.sidebar.text_input("üîë OpenWeatherMap API Key", type="password")
# selected_cities = st.sidebar.multiselect("üèôÔ∏è Select Cities", INDIAN_CITIES, default=["Hyderabad", "Delhi"])
# fetch = st.sidebar.button("üîÑ Fetch Latest Data")
# 
# all_data = []
# if fetch:
#     if not api_key:
#         st.error("Please enter your OpenWeatherMap API Key.")
#     else:
#         for city in selected_cities:
#             lat, lon = get_coords(city, api_key)
#             if not lat:
#                 st.warning(f"Failed to fetch coordinates for {city}.")
#                 continue
#             weather = get_weather(lat, lon, api_key)
#             pollution = get_pollution(lat, lon, api_key)
#             if weather and pollution:
#                 timestamp = dt.datetime.utcnow()
#                 row = {
#                     "timestamp": timestamp,
#                     "city": city,
#                     "lat": lat,
#                     "lon": lon,
#                     **weather,
#                     **pollution
#                 }
#                 save_data(city, row)
#                 all_data.append(row)
#                 st.success(f"‚úÖ Data fetched for {city}")
#             else:
#                 st.warning(f"Failed to fetch data for {city}.")
# 
# # Load Data
# dfs = []
# for city in selected_cities:
#     df = load_data(city)
#     if not df.empty:
#         dfs.append(df)
# if dfs:
#     combined_df = pd.concat(dfs)
# else:
#     combined_df = pd.DataFrame()
# 
# st.title("üåç India Multi-City AQI & Weather Dashboard")
# 
# if not combined_df.empty:
#     latest_df = combined_df.sort_values("timestamp").groupby("city").tail(1)
#     st.subheader("üìä Latest Metrics per City")
#     for _, row in latest_df.iterrows():
#         st.markdown(f"**{row['city']}** ‚Äî üå°Ô∏è {row['temperature_C']}¬∞C | üíß {row['humidity_pct']}% | üå¨Ô∏è {row['wind_speed_m_s']} m/s | üõë AQI {row['aqi_index']}")
# 
#     st.subheader("üìà PM2.5 Trend Comparison")
#     fig_pm25 = px.line(combined_df, x="timestamp", y="pm2_5", color="city", title="PM2.5 Levels Across Cities")
#     st.plotly_chart(fig_pm25, use_container_width=True)
# 
#     st.subheader("üìà AQI Trend Comparison")
#     fig_aqi = px.line(combined_df, x="timestamp", y="aqi_index", color="city", title="AQI Index Across Cities")
#     st.plotly_chart(fig_aqi, use_container_width=True)
# 
#     st.subheader("üî• Combined AQI Heatmap")
#     m = folium.Map(location=[20, 78], zoom_start=5)
#     heat_data = [[row["lat"], row["lon"], row["pm2_5"]] for _, row in combined_df.iterrows() if not pd.isna(row["pm2_5"])]
#     if heat_data:
#         HeatMap(heat_data, radius=25).add_to(m)
#     st_folium(m, width=700, height=500)
# 
#     st.subheader("üå¨Ô∏è Weather & AQI Map")
#     m2 = folium.Map(location=[20, 78], zoom_start=5)
#     for _, row in latest_df.iterrows():
#         folium.Marker(
#             [row["lat"], row["lon"]],
#             popup=f"{row['city']}: {row['weather_desc']} | Temp: {row['temperature_C']}¬∞C | AQI: {row['aqi_index']}",
#             tooltip=row["city"],
#             icon=folium.Icon(color="red" if row['aqi_index'] > 3 else "green")
#         ).add_to(m2)
#     st_folium(m2, width=700, height=500)
# 
#     st.subheader("üì• Download Combined Reports")
#     csv_all = combined_df.to_csv(index=False).encode("utf-8")
#     st.download_button("‚¨áÔ∏è Download All Cities Data (CSV)", csv_all, "multi_city_all_data.csv", "text/csv")
# else:
#     st.info("No data available. Use sidebar to fetch latest readings for selected cities.")
#

# Install dependencies
!pip install folium geopy requests pandas ipywidgets --quiet

import requests
import pandas as pd
import folium
from folium.plugins import HeatMap
from ipywidgets import widgets, VBox, interactive_output
from geopy.geocoders import Nominatim
from pathlib import Path
from IPython.display import display, HTML
import urllib
import os

# --- Setup ---
BASE = Path("/content")
DATA = BASE / "datasets"
DATA.mkdir(exist_ok=True)

# --- Download icon function ---
def download_link(file_path, file_name=None):
    if file_name is None:
        file_name = file_path.split("/")[-1]
    return HTML(f"""
    <a href='/files/{urllib.parse.quote(file_path)}' target='_blank' download>
        üì• <b>Download {file_name}</b>
    </a>
    """)

# --- Functions to fetch data ---
def get_coordinates(city, country):
    geolocator = Nominatim(user_agent="aqi_app")
    location = geolocator.geocode(f"{city}, {country}")
    if location:
        return location.latitude, location.longitude
    return None, None

def get_weather(city, country, api_key):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city},{country}&appid={api_key}&units=metric"
    r = requests.get(url).json()
    if r.get("cod") != 200:
        return None
    return {
        "city": city,
        "country": country,
        "temperature": r["main"]["temp"],
        "humidity": r["main"]["humidity"],
        "weather": r["weather"][0]["description"],
        "lat": r["coord"]["lat"],
        "lon": r["coord"]["lon"]
    }

def get_aqi(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
    r = requests.get(url).json()
    if "list" not in r:
        return None
    aqi = r["list"][0]["main"]["aqi"]
    components = r["list"][0]["components"]
    return {"aqi": aqi, **components}

# --- Build dashboard ---
def build_dashboard(city, country, api_key):
    lat, lon = get_coordinates(city, country)
    if not lat:
        print("‚ùå City not found!")
        return

    weather = get_weather(city, country, api_key)
    aqi = get_aqi(lat, lon, api_key)

    if not weather or not aqi:
        print("‚ö†Ô∏è Error fetching data. Check API key or city name.")
        return

    # Merge data
    data = {**weather, **aqi}
    df = pd.DataFrame([data])

    # Save CSV
    file_name = f"{city}_{country}_pollution.csv"
    file_path = DATA / file_name
    df.to_csv(file_path, index=False)

    # --- Heatmap ---
    m = folium.Map(location=[lat, lon], zoom_start=10)
    HeatMap([[lat, lon, aqi["aqi"]*50]]).add_to(m)
    folium.Marker(
        [lat, lon],
        popup=f"{city}, {country}\nTemp: {weather['temperature']}¬∞C\nAQI: {aqi['aqi']}"
    ).add_to(m)

    # Display Map
    display(m)

    # Display Data Table
    display(df)

    # Display download for this dataset
    display(download_link(str(file_path)))

    # Display all previous datasets
    print("üìÅ Previous Datasets:")
    for f in sorted(DATA.glob("*.csv")):
        display(download_link(str(f)))

# --- Interactive widget ---
city_w = widgets.Text(value="Hyderabad", description="City:")
country_w = widgets.Text(value="IN", description="Country:")
api_w = widgets.Password(description="API Key:")

ui = VBox([city_w, country_w, api_w])
out = interactive_output(build_dashboard, {"city": city_w, "country": country_w, "api_key": api_w})

display(ui, out)

from google.colab import files
import ipywidgets as widgets
from IPython.display import display
import os

DATA_FOLDER = "/content/datasets"
os.makedirs(DATA_FOLDER, exist_ok=True)

# Function to create download buttons for all CSVs
def create_download_buttons():
    csv_files = sorted([f for f in os.listdir(DATA_FOLDER) if f.endswith(".csv")])
    if not csv_files:
        print("No datasets found.")
        return
    for f in csv_files:
        button = widgets.Button(description=f"üì• Download {f}", layout=widgets.Layout(width='250px'))
        def on_click(btn, file_path=os.path.join(DATA_FOLDER, f)):
            files.download(file_path)
        button.on_click(on_click)
        display(button)

# Example usage
create_download_buttons()

!git --version

!git clone https://github.com/USERNAME/REPO_NAME.git

